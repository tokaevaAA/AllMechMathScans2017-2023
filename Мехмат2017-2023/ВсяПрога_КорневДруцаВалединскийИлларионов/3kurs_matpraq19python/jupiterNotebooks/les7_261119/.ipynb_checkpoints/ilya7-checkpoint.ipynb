{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def lda(X, n_topics, alpha, beta, n_iter=10):\n",
    "    n_kw = np.zeros((n_topics, X.shape[1]))\n",
    "    n_dk = np.zeros((X.shape[0], n_topics))\n",
    "    n_k = np.zeros(n_topics)\n",
    "    \n",
    "    docs, words = X.nonzero()\n",
    "    z = np.random.choice(n_topics, len(docs))\n",
    "    \n",
    "    for doc, word, cur_z in zip(docs, words, z):\n",
    "        n_dk[doc, cur_z] += 1\n",
    "        n_kw[cur_z, word] += 1\n",
    "        n_k[cur_z] += 1\n",
    "    \n",
    "    for cur_iter in tqdm(range(n_iter)):\n",
    "        for i in range(len(docs)):\n",
    "            cur_word = words[i]\n",
    "            cur_doc = docs[i]\n",
    "            cur_topic = z[i]\n",
    "            \n",
    "            n_dk[cur_doc, cur_topic] -= 1\n",
    "            n_kw[cur_topic, cur_word] -= 1\n",
    "            n_k[cur_topic] -= 1\n",
    "            \n",
    "            p = (n_dk[cur_doc, :] + alpha) * (n_kw[:, cur_word] + beta[cur_word]) / \\\n",
    "                (n_k + beta.sum())\n",
    "            z[i] = np.random.choice(np.arange(n_topics), p=p / p.sum())\n",
    "            \n",
    "            n_dk[cur_doc, z[i]] += 1\n",
    "            n_kw[z[i], cur_word] += 1\n",
    "            n_k[z[i]] += 1\n",
    "    \n",
    "    return z, n_kw, n_dk, n_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [36:41<00:00, 68.83s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,analyzer='word', binary=True)\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "X_train.toarray()\n",
    "X_train.shape\n",
    "\n",
    "n_topics = 20\n",
    "z, n_kw, n_dk, n_k = lda(X_train, n_topics, 2 * np.ones(n_topics), \\\n",
    "                         2 * np.ones(X_train.shape[1]), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\t1968\tchemical\tdiagnosed\tensures\tford\tmarket\tnixon\ttimmons\tucs\tyeah\n",
      "Topic 1:\t1174\tamps\tavail\tdevices\tgl\tinvolved\tmodems\topen\tpro\tverify\n",
      "Topic 2:\tcards\tdeleted\tgenerators\tgoals\tminutes\trip\tsecondly\tstop\ttrigger\tve\n",
      "Topic 3:\tclassic\tcourse\tgames\thi\thint\thonda\tlaserjet\tsaw\tsoundblaster\tthink\n",
      "Topic 4:\tbd\tbout\tca\tdon\tmonitor\toh\tpark\tresistors\tsincerely\twalter\n",
      "Topic 5:\t685\tbrothers\tdoes\tgraphics\tinternet\tlatest\tlooks\trepost\trunners\tzip\n",
      "Topic 6:\tbrad\tcutting\tdances\tdon\tem\thr\tpointer\treverse\tseat\tslight\n",
      "Topic 7:\t610\t64\tair\tfood\tnecessarily\tpowerpc\tride\tshaft\tsorry\twc\n",
      "Topic 8:\tapplication\tdeletion\tedu\texplosive\tlibrary\tmax\tmotorola\treligious\tve\tvi\n",
      "Topic 9:\t900\tcica\tdsl\tlooking\tmanuals\torientation\tsoon\ttalking\tturbo\ttwm\n",
      "Topic 10:\tchapter\tcom\texplaining\tftp\tken\tmentioned\tnambla\toption\totto\tthanks\n",
      "Topic 11:\t105\t713\tbeer\tbooks\tcompile\tmicrosoft\tmono\tquality\tsan\twondering\n",
      "Topic 12:\t14\t34\tm4\tmb\tmc\tmi\tmn\tmq\tmw\twm\n",
      "Topic 13:\tappreciate\tcheers\tclock\tcompatible\tgets\tlike\tobo\tpc\ttom\tutility\n",
      "Topic 14:\tcadre\tclone\tguns\thi\tisaiah\tjmd\tn3jxp\tproject\tthanks\twondering\n",
      "Topic 15:\t8mb\t95\talive\tcheers\tdeleted\thello\tshort\tshown\tthanx\twritings\n",
      "Topic 16:\t75\tbush\tcheers\tditto\tended\tgoal\toccupied\tproduction\tsale\tsan\n",
      "Topic 17:\tae\tboards\tcanadian\texperience\thearing\toil\tpair\tprice\trank\tsatan\n",
      "Topic 18:\tdoes\tdon\tgood\tjust\tknow\tlike\tpeople\tthink\ttime\tuse\n",
      "Topic 19:\t486\tcica\texe\tfiles\tfunny\tkeller\tlean\triding\tsteve\tticket\n"
     ]
    }
   ],
   "source": [
    "top_words = np.argsort(n_kw, axis=1)[:, -10:]\n",
    "\n",
    "for topic in range(20):\n",
    "    doc = np.zeros((1, X_train.shape[1]))\n",
    "    for word in top_words[topic]:\n",
    "        doc[0, word] = 1\n",
    "    print('Topic {}:\\t{}'.format(topic, '\\t'.join(vectorizer.inverse_transform(doc)[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
