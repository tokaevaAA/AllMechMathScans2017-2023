
\documentclass[12pt]{article}


\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{indentfirst}
\usepackage{misccorr}
\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}

\begin{titlepage}

 \begin{center}
 Lomonosov Moscow State University, \\
 Faculty of Mechanics and Mathematics,\\
 Department of the English Language
  \end{center}

 \vspace{3cm}
 
 \begin{center}
   
  {  Insurance risk models:\\
   3 methods of estimating total losses accosiated with  a portfolio of risks.}
   
    \vspace{5cm}
\end{center}     
   
     
   \hspace{170pt}  {Student: Alexandra Tokaeva, group 309\\}
       
 \vspace{0.1cm}
  \hspace{170pt} 	  English teacher:  V.G. Sementsov\\

\vspace{4cm}

  \begin{center}
  {Moscow\\
  2019}
  \end{center}  
  
  
\end{titlepage}


My name is Alexandra Tokaeva, group 309. My department is the Theory of Probability, where I study different approaches to the problem of modelling  the total losses associated  with a portfolio of heterogeneous insurance risks. \\

 My report is based on the 5th chapter of the book "Insurance risk models" by Harry H. Panjer and Gordon E. Willmot.
 The chapter under consideration gives an overview of 3 typical methods of evaluating total claims generated by our  portfolio,  with respect to  the individual risk model that is widely used in certain applications, especially in life and health insurance.\\
 
So let's onsider a portfolio of n insurance risks. We assume that the risks are statistically independent so that the claim costs or losses of one risk have no influence on any other risk in the portfolio. This assumption may seem appropriate in some types of insurance(for example, life insurance sold on an individual basis). The individual risk model is characterized by the assumption that each risk in the portfolio can experience at most one claim and we also assume that all claim costs arising from a particular risk are amalgamated and treated as a single claim. The total loss assosiated with a portfolio of n independent heterogeneous risks is the sum of the losses from each risk:  
$$ S= X_1 + X_2 +\dots+ X_n$$. 
Thus, taking expectations and using the independence of the $X_k's$, we have:\\
 $$ E(S)= E(X_1) + E(X_2) +\dots+ E(X_n)$$\\
 $$ Var(S)= Var(X_1) + Var(X_2) +\dots+ Var(X_n)$$\\
 
 In general, the distribution of S in quite complicated, but in certain instances simplification is possible. The use of transforms is a substantial convenience in these situations. The Laplace transform of S is\\
$$L_S(z)=E(e^{-zS})=E(e^{-z(X_1 + X_2+\dots +X_n)})=E(e^{-zX_1})E(e^{-zX_2}) \dots E(e^{-zX_2})=\prod\limits_{i = 1}^n  L_{X_i(z)}$$
 Since the Laplace transform uniquely defines the distribution, it may be possible to identify the obtained expression as the Laplace transform of some distribution, which is therefore that of S. The following example illustrates this idea: suppose that $X_k$ has a gamma distribution with density 
 $$f_{X_j}(x)=\frac {\lambda(\lambda x)^{\alpha_j -1} e^{- \lambda x} } {\Gamma(\alpha_j)}, x >0$$\\
 and a Laplace transform $$L_{X_j}(z) = (\frac {\lambda} {\lambda + z } )^{\alpha_j}$$
 $\Rightarrow $ $$L_S(z) = (\frac {\lambda} {\lambda + z } )^{\alpha_1 + \alpha_1 +\dots + \alpha_n }$$
 We recognize it as the Laplace transform of a gamma random variable with parameters $\lambda$ and  
 $\alpha_1 + \alpha_2 +\dots + \alpha_n.$ Thus S is also a gamma random variable with corresponding density.\\

 In most practical applications, however, it is not possible to recognize the obtained expression as the Laplace transform of a particular distribution. \\
 But for large n one may use the central limit theorem to approximate the distribution of S by a normal distribution with mean and variance calculated numerically from  the given distribution.\\
 For small n one can calculate the distribution of S recursively in some instances using the following procedure, known as convolution.\\
 Let $S_k=X_1 + X_2 +\dots+ X_k, k=1,2, \dots ,n,$ denote the sum of first k random variables. We shall assume for simplicity that the distribution of the $X_k's$ is discrete, defined on the non-negative integers. Then the density of the partial sum $S_k$ is \\
 $$f_{X_j}(x)=P(X_1 + X_2 +\dots+ X_n=x); x=0,1,2,\dots$$\\
 Now, $$f_{S_1}(x)=P(X_1=x)=f_{X_1}(x)$$ and for $k>1$ $$f_{S_k}(x)=\sum_{y=0}^x  f_{S_k-1}(x-y)f_{X_k}(y); x=0,1,2,\dots$$\\
 
 Thus one can compute $f_{S_2}(x)$ and then $f_{S_3}(x)$, $f_{S_4}(x) \dots $ until  one obtains $f_{S_n}(x)=f_S(x)$
 This "brute force" method, however, is unwieldy for large n since the number of computations quickly become large, so an approximate method is desirable. It was suggested by de Pril in 1988.\\
 
 His idea was to limit the series defining the desirable density to a small number of first  terms and evaluate the upper bound $\delta$  on the sum of the absolute errors over the entire distribution of aggregate claims. That led to considerable  speeding up in computation without significant loss of accuracy since  the value of $\delta(k)$ can  easily  be calculated and can be used to guarantee accuracy of results when a limited number of terms is used instead of all.\\
 
 {\bf List of terms:}\\
 heterogeneous - \\
 insurance risks - \\
 density - \\
 distribution - \\
 convolution - \\
 mean -\\
 variance -\\
 
 {\bf List of phrases:}\\
 to take expectations - \\
the upper bound-\\
number of computations -\\
considerable speeding up -\\
without significant loss of accuracy-\\


\end{document}



